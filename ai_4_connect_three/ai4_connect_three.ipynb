{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Three \n",
    "\n",
    "The primary description of this coursework is available on the CM20252 Moodle page. This is the Jupyter notebook you must complete and submit to receive marks. This notebook adds additional detail to the coursework specification but does not repeat the information that has already been provided there. \n",
    "\n",
    "You must follow all instructions given in this notebook precisely.\n",
    "\n",
    "Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing. Remember to save your work regularly.\n",
    "\n",
    "__You will develop players for Connect-Three on a grid that is 5 columns wide and 3 rows high. An example is shown below showing a win for Player Red.__\n",
    "\n",
    "<img src=\"images/connect3.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "For your reference, below is a visual depiction of the agent-environment interface in reinforcement learning. The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$. \n",
    "\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Below, we provide some code that will be useful for implementing parts of this interface. You are not obligated to use this code; please feel free to develop your own code from scratch. \n",
    "\n",
    "### Code details\n",
    "\n",
    "We provide a `Connect` class that you can use to simulate Connect-Three games. The following cells in this section will walk you through the basic usage of this class by playing a couple of games.\n",
    "\n",
    "We import the `connect` module and create a Connect-Three environment called `env`. The constructor method has one argument called `verbose`. If `verbose=True`, the `Connect` object will regularly print the progress of the game. This is useful for getting to know the provided code, debugging your code, or if you just want to play around. You will want to set `verbose=False` when you run hundreds of episodes to complete the marked exercises.\n",
    "\n",
    "This `Connect` environment uses the strings `'o'` and `'x'` instead of different disk colors in order to distinguish between the two players. We can specify who should start the game using the `starting_player` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import connect\n",
    "#env = connect.Connect(starting_player='x', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can interact with the environment using the `act()` method. This method takes an `action` (an integer) as input and computes the response of the environment. An action is defined as the column index that a disk is dropped into. The `act()` method returns the `reward` for player `'o'` and a boolean, indicating whether the game is over (`True`) or not (`False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward, game_over = env.act(action=2)\n",
    "#print(\"reward =\", reward)\n",
    "#print(\"game_over =\", game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `verbose=True` when we created our environment, the grid is printed each time we call the `act()` method. You probably might want to set `verbose=False` when you run Q-learning for thousands of episodes. \n",
    "\n",
    "As expected, the `reward` is 0 and no one has won the game yet (`game_over` is `False`). Let us drop another disk into the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward, game_over = env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `Connect` environment automatically switches the\n",
    "\n",
    "The `grid` is stored as a two-dimensional `numpy` array in the `Connect` class and you can easily access it by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_grid = env.grid\n",
    "#print(current_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the grid now appears to be \"upside down\" because `numpy` arrays are printed from \"top to bottom\".\n",
    "We can also print it the way it is printed by the Connect class by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(current_grid[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward, game_over = env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to put another disk in the same column with `act(action=2)`. The environment will throw an error because that column is already filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should throw an IndexError!\n",
    "#env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `.available_actions` of the `Connect` class contains a `numpy` array of all not yet filled columns. This variable should help you to avoid errors like the one we have just encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(env.available_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that column index '2' is missing because this column is already filled.\n",
    "\n",
    "Let's keep on playing until some player wins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward, game_over = env.act(action=3)\n",
    "#print(\"reward =\", reward, \"game_over =\", game_over) \n",
    "#reward, game_over = env.act(action=1)\n",
    "#print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "#reward, game_over = env.act(action=3)\n",
    "#print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "#reward, game_over = env.act(action=0)\n",
    "#print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "#reward, game_over = env.act(action=3)\n",
    "#print(\"reward =\", reward, \"game_over =\", game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the `reward` returned by the `act()` method is the reward for player `'o'`.\n",
    "\n",
    "You can reset the game using the `reset()` method. This method cleans the grid and makes sure that the it is the `starting_player`'s turn as defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.reset()\n",
    "#reward, game_over = env.act(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify existing or add new methods to the `Connect` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "**Your opponent is always the first player. Your agent is always the second player.**\n",
    "\n",
    "For your reference, the pseudo-code for Q-learning is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, Section 6.5).\n",
    "<img src=\"images/q_learning.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Prepare a **learning curve** following the directions below. We refer to this as Plot 1.\n",
    "\n",
    "After $n$ steps of interaction with the environment, play $m$ games with the current policy of the agent (without modifying the policy). Think of this as interrupting the agent for a period of time to test how well it has learned so far. Your plot should show the total score obtained in these $m$ games as a function of $n, 2n, 3n, … kn$. The choices of $n$ and $k$ are up to you. They should be reasonable values that demonstrate the efficiency of the learning and how well the agent learns to play the game eventually. Use $m=10$. \n",
    "\n",
    "This plot should show the mean performance of `a` agents, not the performance of a single agent. Because of the stochasticity in the environment, you will obtain two different learning curves from two different agents even though they are using exactly the same algorithm. We suggest setting `a` to 20 or higher.\n",
    "\n",
    "Present a single mean learning curve with your choice of parameters $\\epsilon$ and $\\alpha$. The plot should also show (as a baseline) the mean performance of a random agent that does not learn but chooses actions uniformly randomly from among the legal actions. Label this line “Random Agent”. \n",
    "\n",
    "Please include this plot as a static figure in the appropriate cell below. That is, compute the learning curve in the lab or at home (this may take a couple of minutes depending on your implementation) and save the figure in the same directory as your notebook. Import this figure in the appropriate answer cell under (A). You can look at the source code of this markdown cell (double click on it!) to find out how to embed figures using html. Do **not** use drag & drop to include figures; we would not be able to see them! Make sure to include the locally stored images in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1431aa87b9e9019a4dbe6e696e0a9082",
     "grade": true,
     "grade_id": "cell-3ac2114f764e8410",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Write all your code for Part 1 within or above this cell. \n",
    "import numpy as np\n",
    "import random\n",
    "import connect\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def grid_2_string(grid):\n",
    "    str = ''\n",
    "    for r in range(3):\n",
    "        for c in range(5):\n",
    "            if(grid[r][c] == ' '):\n",
    "                str += '0'\n",
    "            else:\n",
    "                str += grid[r][c]\n",
    "        str += '/'\n",
    "    return str\n",
    "\n",
    "##plays n games without learning\n",
    "def play_games(table, n):\n",
    "    env = connect.Connect(starting_player='x', verbose=False)\n",
    "    score = 0\n",
    "    \n",
    "    for play in range(n):\n",
    "        env.reset()\n",
    "        \n",
    "        reward, game_over = env.act(random.choice(env.available_actions))  \n",
    "        \n",
    "        while(not game_over):\n",
    "            state = grid_2_string(env.grid)\n",
    "            avail_act = env.available_actions\n",
    "            \n",
    "            for i in avail_act:\n",
    "                if((str(i) + \":\" + state) not in table):\n",
    "                    table[(str(i) + \":\" + state)] = 0.0\n",
    "\n",
    "            max = table[(str(avail_act[0]) + \":\" + state)]\n",
    "            list = []\n",
    "            for i in avail_act:\n",
    "                value = table[(str(i) + \":\" + state)]\n",
    "                if(value == max):\n",
    "                     list.append(i)\n",
    "                elif(value > max):\n",
    "                    max = value\n",
    "                    list = [i]\n",
    "            action = random.choice(list)\n",
    "\n",
    "            reward, game_over = env.act(action)\n",
    "            if(game_over):\n",
    "                score += reward\n",
    "            else:\n",
    "                reward, game_over = env.act(random.choice(env.available_actions))        \n",
    "                if(game_over):\n",
    "                    score += reward\n",
    "                    \n",
    "    return score\n",
    "\n",
    "\n",
    "#def maxq(grid, qtable):\n",
    "#    maxqv = -1\n",
    "#    state = grid_2_string(grid)       \n",
    "#    for i in range(5):\n",
    "#        key = str(i) + ':' + state          \n",
    "#        if(key in qtable):\n",
    "#            value = qtable[key]\n",
    "#            if(value > maxqv):            \n",
    "#                maxqv = value\n",
    "#    return maxqv\n",
    "\n",
    "##checks if non learning games should be played\n",
    "def check_play(inter, n):\n",
    "    if(inter > 0):\n",
    "        if(inter % n == 0):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "##mirrors the action state\n",
    "def reverse_state(state):\n",
    "    reverse_state = str(4 - int(state[0])) + ':'\n",
    "    mini_state = ''\n",
    "    \n",
    "    for i in range(18):\n",
    "        if(state[i+2] != '/'):\n",
    "            mini_state += state[i+2]\n",
    "        else:\n",
    "            reverse_state += mini_state[::-1] + '/'\n",
    "            mini_state = ''\n",
    "    return reverse_state\n",
    "\n",
    "\n",
    "def get_performance(n, k, a):\n",
    "    env = connect.Connect(starting_player='x', verbose=False)\n",
    "    run_run_score = np.zeros(shape=(k))\n",
    "    qtable = {}\n",
    "    for ag in range(a):\n",
    "        epsil = 0.2\n",
    "        alph = 0.1\n",
    "        inter = 0\n",
    "        run_score = np.zeros(shape=(k))\n",
    "        qtable.clear()\n",
    "        while(inter <= (k*n)):\n",
    "            env.reset()\n",
    "            current_grid = env.grid\n",
    "\n",
    "            reward, game_over = env.act(random.choice(env.available_actions))\n",
    "            \n",
    "            curr_state = str(random.choice(env.available_actions)) + ':' + grid_2_string(current_grid)\n",
    "            \n",
    "            while(not game_over):\n",
    "                ##initiales new grid and the available moves\n",
    "                state = grid_2_string(env.grid)\n",
    "                avail_act = env.available_actions\n",
    "\n",
    "                ##makes sure all action states an their reverses are in the qtable\n",
    "                for i in avail_act:\n",
    "                    if((str(i) + \":\" + state) not in qtable):\n",
    "                        qtable[(str(i) + \":\" + state)] = 0.0\n",
    "                        qtable[reverse_state(str(i) + \":\" + state)] = 0.0\n",
    "\n",
    "                ##chance for random\n",
    "                if np.random.random() < epsil:\n",
    "                    action = random.choice(avail_act)\n",
    "                ##else find largest q value from availble moves\n",
    "                else:\n",
    "                    max = qtable[(str(avail_act[0]) + \":\" + state)]\n",
    "                    list = []\n",
    "                    for i in avail_act:\n",
    "                        value = qtable[(str(i) + \":\" + state)]\n",
    "                        if(value == max):\n",
    "                            list.append(i)\n",
    "                        elif(value > max):\n",
    "                            max = value\n",
    "                            list = [i]\n",
    "                    action = random.choice(list)\n",
    "                chosen_state = str(action) + \":\" + state\n",
    "\n",
    "                ##ai and rand make a move\n",
    "                meward, game_overa = env.act(action)\n",
    "                reward, game_overb = env.act(random.choice(env.available_actions))\n",
    "                \n",
    "                if(meward > 0):\n",
    "                    reward = 0\n",
    "                \n",
    "                ##finds maximum obtainable q value from the chosen state\n",
    "                #maxqv = -10\n",
    "                #for i in range(5):\n",
    "                #    key = str(i) + ':' + grid_2_string(current_grid)       \n",
    "                #    if(key in qtable):\n",
    "                #        value = qtable[key]\n",
    "                #        if(value > maxqv):            \n",
    "                #            maxqv = value                \n",
    "                \n",
    "                #new_q = qtable[chosen_state] + alph*(meward + reward + maxqv - qtable[chosen_state])\n",
    "                \n",
    "                new_q  = qtable[curr_state] + alph * (meward + reward + qtable[chosen_state] - qtable[curr_state])\n",
    "                qtable[curr_state] = new_q\n",
    "                qtable[reverse_state(curr_state)] = new_q\n",
    "                curr_state = chosen_state\n",
    "\n",
    "                inter += 1\n",
    "                \n",
    "                if(inter > (k*n)):\n",
    "                    break\n",
    "                if(check_play(inter, n)):\n",
    "                    run_score[int(inter/n)-1] = play_games(copy.deepcopy(qtable), 10)\n",
    "                \n",
    "                game_over = game_overa or game_overb\n",
    "            epsil*=0.999\n",
    "            #alph-=0.1*(1/(n*k))\n",
    "\n",
    "        for i in range(k):  \n",
    "            run_run_score[i] += run_score[i]\n",
    "        #print(qtable)\n",
    "        #print('next')\n",
    "    return run_run_score/a\n",
    "\n",
    "##plays games using random moves\n",
    "def play_rand(n):\n",
    "    score = 0\n",
    "    env = connect.Connect(starting_player='x', verbose=False)\n",
    "    for play in range(n):\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        while(not game_over):\n",
    "            reward, game_over = env.act(random.choice(env.available_actions))\n",
    "            if(game_over):\n",
    "                score += reward\n",
    "            else:\n",
    "                reward, game_over = env.act(random.choice(env.available_actions))        \n",
    "                if(game_over):\n",
    "                    score += reward           \n",
    "    return score\n",
    "\n",
    "\n",
    "def get_rand_perf(k, a):\n",
    "    run_run_score = np.zeros(shape=(k))\n",
    "    for ag in range(a):\n",
    "        run_score = np.zeros(shape=(k))\n",
    "        for i in range(k):\n",
    "            run_score[i] = play_rand(10)\n",
    "        run_run_score += run_score \n",
    "    return run_run_score/a\n",
    "\n",
    "\n",
    "def plot_graph_rand(x_axis, y_axis, y_axis_rand):    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    line1, = ax.plot(x_axis, y_axis, 'b-', label='Learning Agent')\n",
    "    line2, = ax.plot(x_axis, y_axis_rand, 'r-', label='Random Agent')\n",
    "\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_graph(x_axis, y_axis):\n",
    "    plt.plot(x_axis, y_axis, 'b-')\n",
    "    plt.show\n",
    "\n",
    "#n=1500\n",
    "#k=100\n",
    "#a=20\n",
    "#rand_perf = get_rand_perf(k, a)\n",
    "#perf = get_performance(n, k, a)\n",
    "\n",
    "#x_axis = np.zeros(shape=(k))\n",
    "#for q in range(k):\n",
    "#    x_axis[q] = (q+1)*(n)\n",
    "#plot_graph_rand(x_axis, perf, rand_perf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "980d73fb62fae59d610abc96121f71bc",
     "grade": true,
     "grade_id": "cell-ce1405b859519f91",
     "locked": false,
     "points": 60,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "(A) [continued} Insert your static learning curve here (Plot 1).\n",
    "\n",
    "<img src=\"images/perf_graph_2.PNG\"/>\n",
    "\n",
    "\n",
    "(B) In 3 sentences or less, explain your conclusions from the plot above. How close does your (average) agent get to the best possible level of performance? How efficiently does your (average) agent learn? \n",
    "\n",
    "The average agent plateaus at about 6 points after about 60,000 interactions, this is less than ideal but it does mean it is potentially winning 8 out of the 10 games (and losing the other 2). The code runs slower than I would like, I suspect this may be due to the large dictionaries required\n",
    "\n",
    "\n",
    "(C) In five sentences or less, explain the key aspects of your implementation. How many state-action pairs do you represent in your Q-table? Describe and justify your settings of $\\alpha$ and $\\epsilon$. Are there any things you tried out that are not in your final implementation?\n",
    "\n",
    "Every state is mirrored so all possible states are added twice as fast as they would otherwise. \n",
    "Epsilon starts off relatively large at 0.2, this is so that the agent explores as much as possible when first starting out, but as the estimates get better epsilon decreases so the agent realises more on the accuracy of the q values.\n",
    "Alpha is initially set to 0.15. I have experimented with increasing over time as the estimates get better, so they are weighted more than the current value, but the results were similar so I decided against it.\n",
    "\n",
    "\n",
    "(D) In the cell below, make it possible for us to produce from scratch a learning curve similar to Plot 1 but for a single agent, for a $k$ value of your own choosing. You do not need to include the baseline for random play.  This code should run in less than 30 seconds (ours runs in 2 seconds). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e65915a61d304027e4fbd2e714c4beba",
     "grade": true,
     "grade_id": "cell-e0e01e05236aee45",
     "locked": false,
     "points": 40,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXuUXFd15r/dDz2t1qP1VrUs+SHbsuVnS1SbxCuD8YCJMRBgQQIDhhAlgLHlNVnB4DWTzErwSsxMJAMJRs5giAPmIcxLAQImmDDQJakly5aE9bIsq9uypFbLUrvbsqRunflj10ndrr7ve6ruubf2b61aVV2Pe3fdvuerfffZex9SSkEQBEHID01pGyAIgiCYRYRdEAQhZ4iwC4Ig5AwRdkEQhJwhwi4IgpAzRNgFQRByhgi7IAhCzhBhFwRByBki7IIgCDmjJY2dzp49Wy1ZsiSNXQuCIGSWrVu3HldKzQl6XyrCvmTJEvT09KSxa0EQhMxCRC+EeZ+EYgRBEHKGCLsgCELOEGEXBEHIGSLsgiAIOUOEXRAEIWcYEXYiuoeIdhHRTiJ6jIgmmdiuIAiCEJ3Ewk5EiwDcBaBTKXUVgGYA7026XUEQBCEepkIxLQAmE1ELgCkADhvariAIlvD97wOHDqVthRCGxMKulHoRwP8GcAjASwBOKaV+Wv0+IlpNRD1E1NPf3590t4Ig1JGREeCd7wT++q/TtkQIg4lQzEwAbwOwFMBCAFOJ6P3V71NKrVdKdSqlOufMCayIFQTBIl56CRgdBUqltC0RwmAiFPNGAM8rpfqVUucAPA7gRgPbFQTBEvr6+H7XLuDUqXRtEYIxIeyHABSJaAoREYCbATxrYLuCIFhCby/fKwVs2ZKuLUIwJmLsmwBsALANwI7yNtcn3a4gCPagPXZAwjFZwEh3R6XUXwL4SxPbEgTBPvr6gMmTgaVLge7utK0RgpDKU0EQAunrAzo6gK4u9tiVStsiwQ8RdkEQAunrAwoFoFgETpwA9u9P2yLBDxF2QRACcQo7IOEY2xFhFwTBl9FR4PBhFvbly4G2NplAtR0RdkEQfDlyhMW9UACamoBVq0TYbUeEXRAEX3SqY6HA98Ui8MwzwPBwejYJ/oiwC4Lgixb2jg6+LxbZg5f16O1FhF0QBF/cPHZAwjE2I8IuCIIvfX3AxIlAezv/3d4OXHqpCLvNiLALguCLTnUkqjxXLHLKoxQq2YkIuyAIvvT2VsIwmq4u4OhR4IUX0rFJ8EeEXRAEX7TH7kTi7HYjwi4IgifnzwMvvljJiNGsWAFMmSIVqLYiwi4IgifHjvGyeNUee0sLsHKleOy2IsIuCIIn1amOTopF4KmngNdeq69NQjAi7IIgeBIk7OfOAdu21dcmIRgRdkEQPNFL4nkJOyDhGBsRYRcEwZO+PqC1FZgzZ/xr8+cDS5aIsNuICLsgCJ7oVMcmD6XQhUqCXRgRdiKaQUQbiGg3ET1LRF0mtisIQrq45bA7KRb5Pc7FroX0MeWxPwjgJ0qpywFcA+BZQ9sVBCFFgoS9q+zCbdpUH3uEcLQk3QARtQG4CcAdAKCUOgvgbNLtCoIf+/YBF1/sHSLIAufOAYcO8fewEaVY2N/5Tu/3XHstNwh7/HFg+vTxr194ITcMywI7d/KiIm50dgIzZtTXniQkFnYAFwHoB/AIEV0DYCuAu5VSY9rwE9FqAKsBYPHixQZ2KzQqhw8Dl18OfPvbwB/8QdrWxOcLXwA+/Wng+HFg6tS0rRnP8ePA2bP+HvuECey1f/3rfKumrY2309paOztNMDQEXHcdF2O5cccdwCOP1NWkRJgQ9hYA1wP4hFJqExE9COBeAP/D+Sal1HoA6wGgs7NTesIJsTl6lEvd9+xJ25JkPPkkF/ccOwYsXZq2NePxS3V0smED8KxL8PU//gO47z5gxw7g+uvN22eSl19mUf/kJ4Hbbhv72po1wP796dgVFxPC3gegTymlo2wbwMIuCDVBL8mmhSeLKFVJExwYsFPYq1dO8qK9Hfid3xn/fEcHC3upZL+w63PqmmvGf5crrgB+9av625SExBFKpdQRAL1EdFn5qZsB/DbpdgXBCz0Is5yJ8fzz7KkDLOw24ld1GobFiznXPQvpkPqcuuCC8a8VCtwI7fz5+tqUBBMeOwB8AsDXiGgCgAMAPmRou4IwjjwIu7Oo58SJ9Ozwo6+Pm33NnRvv80ScDpmFAqahIb53m+soFDhMc+wY/1BlASM5BUqp7UqpTqXU1UqptyulXjaxXUFwQw/CrAt7czM/ttljX7iwYmccuro4Pn38uDm7aoF2FtyEXYeisnS+ZThZTGhU9CDs789uZ8Hu7koOuM3CHjcMo8lKPxk/YdfHQIRdEGrIsCOR9vDh9OyIy+nTwPbtwO/+Lud+2yrsbkviReWGG9jjz4qwe8XYARF2QagpOhQDZDMzZts2jtkWi8CsWXbG2HVxUlBGTBBTp3Kmie3C7hdjnz2b8/WzdK6JsAuZw+mxZ8mL0ugskWKRUwVt9NhPnOAwV1KPHeDvuWkTMDqafFu1wi8U09QELFqUrXNNhF3IHMPDwOTJ/DhLg01TKgEXXcTZJrYKe9JURyfFInvEv7U4CXp4mLN49HlVTaGQrXNNhF3IHMPDLIrTp2drsGlKpcqkYqMIO2B3OGZ4mL11IvfXRdgFocYMDfEg7OjI1mAD2N4XX6yIna0xdpPCfskl/ANmc6GSPqe80OeaykgzFBF2IXMMD3P2Qta8KGBsfB1gwTt50rv5VFr09XFs2URBTpRCpcHBdMRTe+xeFArcEM32fHyNCLuQOfQgzKKwl0rApEmcKQKwsAPchMomenu5OKnFUG16sciNwk6e9H7Pnj0cYtu40cw+o6CdBS+ylvIowi5kDqewHz3KnlRWKJU4t3vCBP5bC7ttcXYTxUlOwizIsW4dcOYMsHu3uf2GJYzHDmQn5VGEXcgcOh5aKPBle1aKlM6eBbZurYRhgMYR9pUrOSTjFY45cQL46lf58dGj5vYblqAYu3jsglBjnDF2IDuDbft29ki7HCsCz5rF9zZNoOriJJPC3tYGXHmlt7CvX88VuVOmpCPsQR773LkclsrKuSbCLmQOZygGyM5g06Jmu8d+6hQfY5PCDvAP2qZN49vfnjvHq0m98Y3AVVdV2hnXk6AYe3Mzzzlk5VwTYRcyhVIVYc9a173ubhbLRYsqz9ko7CZTHZ0UizxJvHfv2Oc3bOAU0DVr2DO2MRQDZCu9VoRdyBRnz3Jq4NSpfHk/bVp2BpuzMEnT1sbeoI3CnrRPTDVuhUpKAWvXApddBtx6KzBvXnoee5CwZykLS4RdyBTVXfgKhWxkKhw5Ahw8ODa+DvCEom1FSmHXOo3K5ZdztbBT2Lu7gS1bgLvv5rx5Lez1XK3IeRXohxb2LBQpibALmaK6WVNWvCi3+LrGtrYCfX38g7NggdntNjUBr3vd2ArUtWuBmTOBD3yA/547l5uF1TOv/7XXWKz9YuwAn2unT9v1I+yFCLuQKbIs7K2t7os62yjs8+ezvaYpFoGdO4FXXuErmMcfB1avrvw/583j+3rG2f1a9jrJ0mS9CLuQKaoHYaEAvPQSZ1bYTKkEXHcdV51WY6Owmw7DaLq6OMzS08OZMETAnXdWXtfrq9Yzzu7XstdJQwo7ETUT0VNElEJBsNAouMXYleIYtq2MjHAc2S0MA9gXY6+lsK9axfc//Snw8MPAu989dl9peOwi7P7cDeBZg9sThHFUD8IspDzu2AG8+qq3sNvosZvOiNHMmsUZMGvXcsOve+4Z+3qaHntQjH3BAs5gsvlc0xgRdiIqAPh9AP9kYnuC4IVbKAawOzPGb+IUYGE/fZpvaTM4yLdaeewAH4czZ4Abb6x48Jr2dp5ktTHG3tzM4h5X2EdH/ZugmcSUx74OwF8AqGOSUjJe/3ouY84ye/eyh7NnT9qWVFCKOxfqvh+mcQvFAHZ7UZs38/9pyRL3120qUjp0iO9rKew33sj3a9aMf62pCZgzx84YO5AsvXbHDs4A+sEP4n0+ComFnYhuA3BMKbU14H2riaiHiHr6+/uT7jYRSnHK1bZtqZqRmK1bgf5+4Cc/SduSCgMDwDPPAL/+dW22Xz0IZ8zg/iI2C/vhwyzqXqvz2CTsW8uj+Oqra7eP978fePRR4J3vdH993jw7Y+xAsiwsnea5YkW8z0fBhMf+egC3E9FBAN8A8AYi+pfqNyml1iulOpVSnXPmzDGw2/gMD7O4Dw6makZi9Alm05Jj2qZaCW31ICSyP+XxxIlKsy83bGoEVipxNewVV9RuH1OmsLg3eajP3Ll2xtiBZEVKpRL/aHlduZkksbArpT6llCoopZYAeC+Af1dKvT+xZTVEC7oIu3lqLexDQ+MXHbZd2AcGKl65GzZ57N3dXETkJbr1oN4ee9gYO8Dn2vAwN0qLim4p4XXlZpKGzGPPm7AfPGhPul89PPbqRYdtb86UFWEfGuI4sNckb71Iy2MPI+xxs7AGBnhOrF7H1qiwK6WeVErdZnKbtSBPwj5zJj+2xWvXJ/zLL1cGjEncenoUChzHHh01v7+knDvH55mfsOtQTNrC3tPDxUNpC/u8efx/rsX548bwMGe86FWt/Ig7Wa9XjqruFVQrxGPPML293BGvtdUeYXdmDNTCi/YS9tFRe65anOieJ37CPnky39KOsevJvde9Ll076p3LPjTE8fUwIZK4wl4qcXirszO6fXFoSGHX8bE4cTJbOHeOheySS7hU3RZh7+tj70c/No1b32ybUx61F+43eQrYUaRUKgHLlvn/CNWDelefhunsqFmwgH8AoqY8dndzplHY/SSlIYXd6bFnoQWnGy+9xLZ3dPCl85YtXLqeNn19nMeuH5vGbaWbLAh7kFimLexKufeLT4N6e+xRhL21lRukRTnXRkc5FFPPY9vQwj4ywi07s4hzlZuuLi5Z37EjXZv0Wpn6Ur6eoZha7S8pWRH2gwdZSOsVA/YjDY89TKqjJmoW1u7d3M2ynse2oYW9+nGWcAq728o0aXDyJP/AXHopVw/WS9jb27lroo3CruPmQcKediMwHV9vRI89zLJ4TqIKe1BLiVogwp4DYb/wQvZynAsYpG1TrVY2chuENhcpZSXGXirxcb3qqvRs0EycyCst2RhjB6Kn13Z38///0kuj2xYXEfaMCntvL5+M06ezsHV1pe+xO5dUq5XQel0227pE3sAA0NLCa7P60d7OHntacz6lErByJdtqA/Vc+zSqsBcKlWZpYahnYZJGhD2jwq57ZuuTpVgE9u1L1+ur9tjrFYrR+7TVY29vDx7U7e08yZZGptbp08BTT9kRhtHMnWt3jB0AXnwx+L2nTgG//W39j23DCrv2TLIs7M6e2TbE2fv6OFd3/nw++QcGzLai9Vt0uFDggRZ1EeTvfQ+45ZbarcAUVHWq0e9JI86+bRsnEtgwcaqpp8ceJ8YOhLtC3LyZz1sR9jowOAgsXFh5nEWqV7np7OT88bSFXa+VGcWrCcvp0zxIvIR9ZCS6GPzwh8ATTwDf/rYZG6s5cSKcsKdZfarPmbQLk5zU22OPI+xhrhBLJb5aq+47X2saVtj1PyeLwj4ywnnsTmGfOpULINIWdn0VUYsURL8ufHH3t28f369dW5v49sBA8MQpkG6/mFIJWLq0kmZoA/Pm8bGodW3G+fPsMEQRdu0UhhX25ct5LqyeiLBnUNiPHuV4bPViCMUiF0Kk1TPFeRWhBd7khKZfs6a4wr53Lw+6nh7gN79JZp8bUUMxaQh7d7ddYRigkvJY66UbXn2V76PE2CdOZPuCzrU0i74aVtjnzOGQQRaF3Zl94qRY5EKIZ1NYeVYptkvbtGgR39fCY3cT9jhd9wYH+Ufyrru4mdratcltdKKU/cLe18fhMpsmToH6FSlFadnrJEzK4759HIoTYa8DeoGN6dP5lkVhd2afONFeVxrhmMFBHiTapilTOARhUtj9BuHs2dydL8oVgg7DXHst8Kd/Cnz3u1yBaYrTp3ltzzDCPmMG39d78jSN4pkw1KtIKUrLXidhsrD0sU3jaqjhhP30aY7bTZ/OK8VksRGYPqGqV5K/5BIW0zSE3e3HxnQKol+MvamJrxKi7E8L+7JlwMc/ztv4/OeT26kJW5wEcJbWjBn199i7u7lqV/f3sYV6eexJhD3Iiejurv1qVF40nLBrD72tjW9Z9dgnT670YtcQseeVRgVqPYXdaxBG3d/evXx/8cX82Xe/G/inf+JwlgnC9onRpFF9WioBN9wQrhd5PamXx66vAqPE2AE+X06erHzejVKJs2HSWI1KhD2jwu4sTnLS1cUFESdP1t8mYOxVhGlhD4qHRt3fvn3A4sWVZfbWrOHz4ZFHktmpsV3Yz57lxattC8MAPDYnTrTbYwe803mHh3lR97QmpUXYMyzsbuhBumVL/ewB2CYi7let6ehgj+vMGTP7COuxh01b3Lt3bP+OVauAG28EHnzQTGZR2AZgGt1WoF48/TT/b2zLiAH4XKrHEnlJhd3LkUh7NSoR9gwKuzP7pJpVq3hQ1Dsc09vLcVHnJb3pIqWg1eQ7OtgLPX48eFtKsbAvWzb2+XvuAQ4cADZuTGYrEN1jnzWrvh67TR0d3ajHotZxhT0oCyvt1agSCzsRdRDRL4joWSLaRUR3mzCsVmRd2EdHeX1PL2FvawOuvLL+E6huVxGmi5TCeOxAuMyYgQEOV1UL+9vfzt0yTaQ+Rpk8BeofiimV+Jjp1FTbqEdbgbgx9qB03rRXozLhsY8A+O9KqSsAFAF8nIiWG9huTci6sB87xlk9XsIOsAdWKtW3U2A9hH1oyH/R4Sj70xOn1a1UW1qAT3wC+OUvuTFWEgYG+Edo4sRw729v5/OxVn1rqimV7AzDaOrRViCuxz5pEqfYup1rNqxGlVjYlVIvKaW2lR+/AuBZAJb6AOOF/cwZczHgeuCV6uikWORFlHU6Xz1wE3bTRUq6C59Xp8Qowu5Mdazmj/+YB/q6dfHs1IQtTtLo9+oFsGvJ0aPA88/bG4YBKh57LR2UuMIOeKc8HjzIxzfNY2u0+zIRLQFwHYBNJrdrEi3s06axsAOc3hbWq6oljz7KovSpT3m/x6s4yYk+obq73YXLjcce8w4/LFgAfOtb3sfolVe4HqD6x2baNK4XMCnsfgNw7lz2uMOEYvbuZe9/yZLxr82YAXz4w8BDDwEPPBC/h0rYBmAaZyMwne4Xlr/5G+AHPwj/fh2CsFnY587lq5eTJ8en9moeegj48pfdX1u2jMeUX8vk4WE+r+P0oS8UgF/8YnyDL10bk2mPXUNEFwD4DoA1SqlxAQ4iWk1EPUTU01/rBhA+DA7yP3LixIqw2xKOeeQR4P77/TMywgj7FVfwd4sSZ//iF9nTmD177K2piQVj61bvz+rJUTebTC6AESTszc3AZZdxmlkQ+/YBF13EbSXc+MAHWFSefDKWqQDie+xx4uyPPsr/h+r/n9dtyRLgjjt4cQ1b0T+ofnH2z32O55yqv9/QEPC1rwXXJERt2evkT/4EuOmm8fu++GLgIx/hpnxpYcRjJ6JWsKh/TSn1uNt7lFLrAawHgM7OzpTWiWER14Jum7D39fGJ9tvfAitWuL+nt5djzLNne2+nqYln48MK+7lznJ71Z38G/P3fj33tyBH22EslTgX0sglwF/aoy4j5EWYQFovcGkApf0+tOtWxmmuu4fz2Ugl4z3vi2Tsw4B8yqyaJsA8OArffDnzpS9E/ayv6quXoUf7BrubkSe6L9JnPAJ/+9NjXHnmEr7oGBirj3I2oLXud3H4732zERFYMAfi/AJ5VSv190PvTxinsupWmDcKuVEUA/VIV/YqTnBSL7Ln6VcZpnnmGWy24XTrOn8/eXZBNgLfHbjrG7kexyCEQv/kFpfh1vzBVayv3uE+SNlpPj915XueFII99Uzng63behj2WSYTdZkyEYl4P4L8BeAMRbS/f3mJguzXh1Ck7PfYTJyqrDfl52n7FSU66urhAoqcn+L1BjaB0lo2fTYB72lyhwB7X2bPBdgQRZhCGaYR2+DC3aw1aXLhY5MyYOJPr58/zJGicGHvUIqWREf4+eRN2p8fuRqnEV6du4aSwx3JoKHqqYxYwkRXz/5RSpJS6Wil1bfn2IxPG1QLd2RGoDAQbGoFpcZwwIVhEw1ze6wmdMOGYUokXD/DablcX79fL8+7r40HoNrlaKLCH/NJLwXYEEUbYw8wv+GXEOOnq4h+kOGmPp06xuEcR9mnTeBIvqseu48h5E/bZs/nK1MtjL5WAq65yXyhcPPYGw9YYuxbNN72J44ZuvV7On+cJsjAee3s7C1eYUEJ3t/8q6tqT3+SR6+R3FWEylz1MjL2piX/U/L63zmEPEnZdNRgnHBO1OAng4x+nSMmZwpsnmptZ3N089vPn/XPFRdgbDNuF/d3v5ns3ET1+nD3IMMIOsMcZVKjU3w8895x/ata117I37iVwYYTdRGZM2NXku7p43kDnKFezbx8XmAQdx4ULuUlYnCreqO0ENCLsY/GqPt27l50fr/M27BqyIuw5wSnskyezV2CLsDc3A299K3tubmLil33iRrHIg8Jv8Qj9A+JXgThhArd29RI4v941Jj32sIOwWPSfX9i7l3vXh2mnqn8coxK1AZgmTiOwPAu7V/VpUJ+blhYOuUqMvUFwCjuRPW0F+vo4rXDGDI4buolJmBx2J85CJS+6u3kQXH998La2bh0/CTo8zJOEXjZNn84x0KTCPjoaftHhoBBKUKqjk2IROHSIJ1yjENdjj9MILM/C7uWxl0o8VtzSIDVhjqV47DngzBkWJucAsEnYtThqL/H8+fHvAcIL+1VX8Unr53GWSpyzPWWK/7aKReC117jVqxO/4iSNiZRHvehwmEHY3s7C7fa9R0c59BS2Ilf/OEb12iUUYwYvj71U4h9wv6uuMMdShD0HuA0AW4TdGc4oFjl+qCf5NH19nF8dtty8pYVTwbxEaXQU2Lw5XCMorzTCML1rTAh7UMvearzmF154gQuywnrs110XnKnkxsAAXxHqDKywaDGK0h9FZ3XlUdjnzeOsH50KDPDfO3cGl+wHCfu5c+zoibBnHFuFXRcnOYUdcBfRRYuiLbWlc7GdA0OzaxfHGMP0tNDtXb2EvdYee9RmTcUie3rV8wthUx01EydymCqOsM+cyfMmUWhv5ytLt/+XF3n32IGx4ZgtW/hqNsghCZqviOosZAkRdguE/dQpPsm013vZZRw/rI4Rhy1OclIscgHLtm3jX4u6Qr3beqp+xUmaQoHz2EdGwu3HjTjCDowX5LCpjtXb6umJ1k43agMwTdhsDieDg3x1kEfP0636VP9Pq5tvVRMUY0/S2dF2RNgtEPZqr9er14tf9okXfjHiUgmYM4ebYYWhq4tbvTpjnr29LGB63VA3CgX2sJIUKQWtd1rNihU8b1D9vfft48ncKN0Tu7rYgw7TXEwTtZ2AJk5bgcFB/k5pLJpca7SwO8+57m7g8su9Oz5q2ts5pOnlUIiw5wQ3YZ8+3T5hB1iQd+6sVBVWh2vCMm8esHSpe4ZIUGFSNW6FSmFsMpHyGPWyWc8vVH9vvRxe2O8MxJtAHRiIVpykiSvsUWP5WaE6FBNlEYug/vYi7DkhKx47MD4Xe2CAY69RhV1vq1qUXn4Z2L07Ws/o669nwXRuK4ywB60PGYY4g9BtfmHfvvATp5qODk5FjVKBWm+PPY/xdWB8v5gDB7hQL8yEvz6WXnH2uMviZYGGEna37IG2Nk6lSxL/TUpvL3uQCxZUnqvOxQ6TfeJFVxenJTqFdfNmvo8i7JMncxWqU+DC9K4x4bFHDcUA/L1HRiq9Xs6e5cnUKPF1gP83UQuVksbYoxQp5VnYJ0/mMJMW9igLcAfNV4jHnhO8PHbna2nQ18ftcZ2LPsycyXFELSZRc9iduBUqdXd7d8bzo6uLsxJGRjiv/fjxYJtmzOB4d7099uofxwMH+CooqscO8DF87jluwRDE2bMcQhOP3QzOIqVSic+BK68M/lzQsRRhzwmDgyyekyZVnrNF2N28Xuei1EmE/Zpr+Ds7PU6/znh+FIs8IHbtClecBLDHm3QlpTipabqXvP7ecTJiNFHi7HHbCQCcXjl1qgi7E2eRUqnE2TBh0kiDhD3OVWBWaDhhb2sbO3Fmi7C7iWNXF3uIBw6wKDY3x1t/s7rXy/nzPAEaZ01Gp/cfpXdN0lz2uN6VM4Sic9jjeOw33DB+fsGLOJ0dnUStPs27sGuP/dVXufI5THwdCI6xSx57TnAbADYLu9NL7OvjboNRC16c29K9Xvbs4TSwsAPEydKl7EFpm4D6CPvQEP9ARV10uFis9JLfu5fbwAalybkxZQpf+UQR9jgeu/6cxNgraI9961YOAYZ1SNraeLxIKCbn2Cjsg4N8cxPHK6+s9HqJk+ropFjkrJrt26MXJjkhqoSIogh7Rwc30vJbqNuPuD09nD+OOtUxLsUiTzoHfYckoRggWiOw8+c5np9nYZ83j+dyfv1r/lvPnQRB5H8stbAH9UnKIiLsKQu7nzg2N1cWjQi7cpIXToHTnfHiilyxyF7/M8+w9xtGcAsFFkSvZc6CCNuLvRrdS75Uipfq6KRY5CuHXbv832fCYw8r7DpOnGdhnzuX55k2bgQuvjhacZnfsRwaYlHPY2FXDr+SN1kTdoDDJU8/za1jk3jshQLfursrhUlxT2gdwtm4MbxNSRfciOux6/mFJ57gyd4kHnuY9VSB+sbY89wATKPnlX7zm+hXmX7HMq+dHQERdmuE3csb171eXnstmbDrbf3iF+E64/nR2ck/Cq+8El3Y48bZwyyL50WxWGk3nMRjv+gijtGHEfYJE+Lb297OBWTVbZvdyHMDMI320JWKJ+x+k6ci7D4Q0ZuJaA8R7Seie01ssxa4CfvUqRyLS1vYFy50f90ZTzQh7EePxhsgTi64gHuxRLEpqbAnGYTO75rEY9fzC0EVqLo4KUrbAiezZrGoh1lkvRGE3ZkJFvW8DYqxi7B7QETNAP4BwK0AlgP4QyJannS7tcBN2Jua0m0r0NfHJ+4qyQ9tAAAWlElEQVSECe6vz53LcUUgubA7s2DCTkAFbSusTe3tHOtOIuxx09Kc3/uSS+Jtw7mt3bu9+48A8dsJaKIUKTWCsGuPfdIkzkyKQlCMPY+pjoAZj30VgP1KqQNKqbMAvgHgbQa2a5Rz57hniNsASFvYg8RReylJhf2667hA64orePI0CVFt0kVKaXjsupf8okXJPTT9vXVLBjfSEPa8NgED+FydMIFDgM7q7DC0t/O4d+tvn2ePPWJWsCuLADinxPoAjPMHiWg1gNUAsHjxYgO7jYbukmijsAe1zX3Pe7hdrrOXTBwmTwb+6I+SxZk1t9zC6Zg33hj+Mx0d8SdPk8TYAeADH6hkkCRh5Uq+yuvuBt70Jvf3DAwkC/mIxz4WIl7k/eabo3/WWaRUvWbA8HD8CW7bMSHsbpHEcQt7KaXWA1gPAJ2dnREW/jKDX/ZAmsLe2wvcdJP/e976Vr6Z4CtfMbOdhQt5EjYKhQLwq1/F21+SUAwA3H9//M86mTbNe7FxTdwGYJqgikknjSDsALBhQ7zPORuBuQl7Xj12E6GYPgDOnI4CgIhrutcevwHQ1hZuoso0Q0NcAZokPz1LFAqcchgm26MamwZhscgtGdy+h1LJQzFRVlHS53VeY8VJ8bv6kRi7P1sAXEpES4loAoD3AviBge0aJUjY0/DYwzbRyguFAqduOpc5C4Ntiw7rxcb37Bn/2vAw25pE2GfM4PBDWGG/4IL4rSbyjp+w2+QsmCaxsCulRgDcCeDfADwL4FtKqYDavPrjN8mUlrAn6diYReKmPNrW08Ov02PSqlOARXrmzPDCnvcwTBK8wlpKibAHopT6kVJqmVLqYqXUZ0xs0zQ2euwi7OGwrQufXmzcT9iTTsqFbQQmwu6PV1jrzBlucSHCnnGChH1oKH6Dqrhogaue1MkrcZfIs81j14uNuxUqJW0ApgnbCEyE3Z/Jk/lWfSxtcxZMI8LueM5EOlwUenu5RN258EeemT2b85GjCruNCyJULzauMRGK0Z8PI+ynTomwB+F2LG1zFkzTUMLe1OTeojOtfjFJOzZmjaYmvjqJmstu4yDs6uI47ZYtY5+vt7CLxx6MW1jLxnPKJA0l7NWrJ2nSFPZGia9r4lSf2njZvGoV31eHY7QYx1nMw4nE2M3h9iNp41WgSRpO2N3QmTIi7LUnibDbNAirFxvXnDjBRUxevX/CMmsWh3nOnvV/nwh7MG7zFTY6CyYRYUc6Hvvp03yyNaqwqwi1x7Z6V3o9Ved3SVqcpAlTfaqUCHsYJMaeY2wT9kYrTtIUCuyFHj8e/jO2DsJikb/Hc89VnjMt7H5x9uFhFvc8NwAzgQ5rOX+AbT2nTJEpYe/pAb70pXiftU3YGy2HXRMn5dHWy2a3QqV6Cnuj9IlJSns7pzI7x7e+CrTtnDJFpoT9q18F7roLOHIk+mf90sLSEHadGdJIWTFAvCIlWxcdvvJKFgansJ84YaZjoN6GXyhGhD0cbkVK4rFbxF13cd+QL34x+mf9PHb9q52Gx94oxUmaOGufDg1xkYltiw47FxvXiMduH27HUoTdIi69FLjtNhb2116L9lk/YW9uZnGvZ4fHvj72JGzzQmvN3LlAS0t0j93WAajXU331Vb7cf/llEXbbcJuIHh7m1Oe8FgdmStgB4J57gP5+4OtfD/+Z0VH+R/pNMtW7X0wjpjoC/CO6cGF0Ybc1Flos8vm1dSt3fFTKjLBPncopkyLsyXH7kdQte+OuS2s7mRP23/s9Xvdw3brwKXN+qydpRNjrR9Rcdts9doDDMaYagAEsOEFFSiLs4fAKxdh6Tpkgc8JOBKxZA+zYAfz85+E+E2YAiLDXj6jCnnRZvFoyZw4vNl4qmWsApglqBCbCHg69vq8Iu+X84R9yrHbdunDvt03Yz5zhxSYaLSNG09ERrUjJ9kFYLI712E0Je1C/GD0nNG2amf3llZYWFvfqUIzN51RSMinsEycCH/sY8K//6r6KTTW2CXujFidpCgWuvH355XDvtznGDnAF6pEjwFNP8d/1EvbBQc4Wam01s788Ux3Wsv2cSkomhR0APvpRFvgHHwx+bxhhnz69fsLeqMVJmqgpj1nw2AFg40a+NynsQTF2CcOEo/pH0vZzKimZFfa5c4H3vY+LloK64NnmsYuw833YOLvtl81XX82e8+bNnGtvSmx1jN0rZCXCHp7q+QoRdotZs4bzhx9+2P99UYQ9SnOquDRqcZImqrDbPghbW4HOTj53Zs0yV0jV3s59dXQxTTUi7OGp9thtdxaSkugUJKLPEtFuInqGiL5LRDNMGRaGFSuAm28GPv95rkj1Iqyw6wVua01fH4d+GnXSa/58Fr8wwq7/J7bHQ3U4xlQYxrktrzj74KA0AAuLxNij8TMAVymlrgawF8CnkpsUjXvu4cnI73zH+z1a2P3+kfXsF9Pb27gZMQBnKYQtUjp7NhuLDtdS2L1CjeKxh6e9nbOIRkb4b9uvApOSSNiVUj9VSpUPFUoA6h41vvVWYNkyYO1a7zDKqVPsHftdItdT2Bs5h10TNpfd1l7s1WhhN1GcpHFrXuVEhD08zqZq589zCNf2cyoJJmPsHwbwY68XiWg1EfUQUU9/f7+xnTY1AXfeyRNXXqmPYQZAvYT9lVeA3buBpUtrux/bKRTCZcXY2rK3moULgeXLgUsuMbfNMKEYEfZwOI/l6dPsBOZZ2FuC3kBETwCY7/LSfUqp75ffcx+AEQBf89qOUmo9gPUA0NnZaXSK8pZb+L67m5crqyaKsNe6Edgjj7AXescdtd2P7RQKwI9/zAPMr19HlrrwdXdzCq4p/IRdVk+KhjOspR/b7iwkIVDYlVJv9HudiD4I4DYANytVj5yS8SxbxpVl3d3Ahz40/nVbPPbRUeBzn+OCFr0YcqNSKLBonzpVKfl2IyuhGMC8yPqFYl57jePFIuzhcP5ILlzIj7NwTsUlaVbMmwF8EsDtSqlXzZgUnaYmjnFWLyysCZM9UA9h37iRl1G7557a7SMrhE15zJLHbpoJE3huyG3yVPrERMMp7FlyFuKSNMb+BQDTAPyMiLYT0UMGbIpFsQjs3Fnp5OjEFo997Vpg8WLgHe+o3T6yQlRhz/Nlsx9ejcBE2KPhvPppBGchMBTjh1LK4FRRMopFjjtu3sy57U7CCLvOKa+VsD/1FPDLXwKf/Syn+zU6Ydc+bYRB6IdXvxg9FyTCHo5p03jcnTjRGM5CpitPnbzudXzvFo4JI+ytrbyaUa2Efd06FqePfKQ2288aCxbwpGmQsDfCZbMfXsIuHns0dH/7RvHYcyPsM2YAV1wxXtjPn+fwTJgBUKt+MUeOAI89xhO7fhOFjURrK1egBqU8NsIg9MOrEZgIe3S0sDeCs5AbYQcqE6jO3JzhYf47TWH/x3/kDIa77za/7SwTpkipES6b/ZAYuzn0sWyEcyp3wn78OGeeaKIMgFoI++nTvPj2W99qtnglD4QV9jwvOhxEezv3rR8dHfu8CHt09NVPI1wF5krYu7r43hmOSVvYv/51/rFZs8bsdvNAGGHXXfjyuuhwEO3tfMV58uTY50XYoyMx9oyyfDlfXnV3V55LU9iV4hTHa67hRbiFsXR08PH2O+Z5b9YUhFcjsMFBrnI1Wemad5wx9pYWrhPIK7kS9uZmruh0euxR0sJMC/sTTwC7drG33qgepx86l10vFehG3turBuFVfSrtBKIzaxZX7B4/nv9zKlfCDnA45umnuXsbkK7Hvm4dMG8eL74tjCdMkZJ47Hwvwp4cfSwPHcr/OZU7YS8WeaJp61b+O6qwnzplZhWl3buBH/2osjarMJ4wa5/mfaWbIETYzSHCnmF0oZKOs0cV9tFRzmRJyoMPsqB/9KPJt5VXdDMm8di98Yuxi7BHQ4Q9w8yZw2mFOs4eVdidn4nLiRO8yPb73seLbgvuTJzIxydI2PMeD/Vj+nRucicee3K0sL/6av7PqdwJO8DhmO7uSs/qqVN5YjUIU8K+fj17/ZLiGExQymOje+xNTcDMmeOF/dQpEfaoOFe3yvs5lVthP3KEL7mieDa6tW8SYT93DvjCF7gR2YoV8bfTKHR0+At7o8fYAfd+MeKxR8e5Hm3ez6ncCjvA4ZgoA8CEx75hA6fvSc/1cIjHHowIuxkmTqycS3k/p3Ip7FdfDUyeXH9h1wVJy5bxIttCMIUCl8zrakAnetHhvMdDg6huBHbmDHD2rAh7HBphWTwgp8Le2gp0dnKcvZ7C3t0NbNnCzb6acnlkzeOXy94Iiw6HoboRmLQTiI+Os+f9nMqt/BSLvLhFf3/9hH3tWm7L+8EPxvt8I+In7I3Q0yMM1aEYEfb4aI897+dUboW9q4svV/fuDT8Akqyi9MILwOOPA6tX5/+kMYkIezDt7Xwszpzhv/X5GbSOrzAeEfaMowuVgPDCrpsqxRH2z3+e+8HceWf0zzYyYYQ97/HQIKqLlMRjj4/E2CNARH9ORIqIZpvYngkWLuSFo4FoAyBOv5hXXgEefhh417sqa3kK4Zg0CZg9213YG2GlmzBUNwITYY+PxNhDQkQdAG4BcCi5OWbR/dlrLexf+Qp/RlIc4+GV8iihGKa6X4wIe3waJRTTYmAbawH8BYDvG9iWUYpF4JvfjC7sx4+7L0fmhlLcF6ara2z4RwhPoQAcPDj+mB85wvd5H4RBiLCbQ4Q9BER0O4AXlVJPk4UNx2+8ke+dFWdBzJoF/Nu/cXggCvffH+39QoULLwQ2bvQ+5o2+ALjE2M0xfz7fz5yZrh21JlDYiegJAPNdXroPwKcB/NcwOyKi1QBWA8BiHfyuMStXAj/8IXDLLeE/s3Yt8OST0fbT1sbxdSEe994LXH65e7vk2bOBiy+uv0024eaxt7Q07jqwSXjjG4Hvf5+1Ic+Qitl8nIhWAPg5gPKSFigAOAxglVLqiN9nOzs7VU9PT6z9CkKjoRRXUt91F/DAA5x59dhj4cOFQn4goq1Kqc6g98UOxSildgD4z6a0RHQQQKdS6njcbQqCMB6isUVK0idGCCK3eeyCkCec/WJE2IUgTGTFAACUUktMbUsQhLGIxy5EQTx2QcgAzkZgIuxCECLsgpABxGMXoiDCLggZQAu7Xu5RGoAJfoiwC0IGaG8HRka4f4547EIQIuyCkAF086qjR3kBEhF2wQ8RdkHIALr69Pnn+V6EXfBDhF0QMoAIuxAFEXZByABa2A8e5HsRdsEPEXZByAA6xi4euxAGEXZByAAi7EIURNgFIQO0trKYHzjAf4uwC36IsAtCRmhvB/r7+bEIu+CHCLsgZATnSmAi7IIfIuyCkBF0nJ0o/2t2CskQYReEjKA99rY2FndB8EKEXRAyglPYBcEPEXZByAha2KWzoxCECLsgZAQdYxePXQhChF0QMoKEYoSwJBZ2IvoEEe0hol1E9IAJowRBGI8IuxCWRItZE9F/AfA2AFcrpc4Q0VwzZgmCUI0IuxCWpB77RwH8rVLqDAAopY4lN0kQBDckxi6EJamwLwPwu0S0iYh+SUQrTRglCMJ4tMc+bVq6dgj2ExiKIaInAMx3eem+8udnAigCWAngW0R0kVJKuWxnNYDVALB48eIkNgtCQzJ9OnD//cA73pG2JYLtkIsGh/8w0U/AoZgny38/B6ColOr3+1xnZ6fq6emJvV9BEIRGhIi2KqU6g96XNBTzPQBvKO9wGYAJAI4n3KYgCIKQgERZMQC+DODLRLQTwFkAH3QLwwiCIAj1I5GwK6XOAni/IVsEQRAEA0jlqSAIQs4QYRcEQcgZIuyCIAg5Q4RdEAQhZ4iwC4Ig5IxEBUqxd0rUD+CFgLfNhv058VmwEciGnVmwEciGnWKjOWyz80Kl1JygN6Ui7GEgop4wFVZpkgUbgWzYmQUbgWzYKTaaIyt2ViOhGEEQhJwhwi4IgpAzbBb29WkbEIIs2Ahkw84s2Ahkw06x0RxZsXMM1sbYBUEQhHjY7LELgiAIMbBO2InozeXFsfcT0b0p2XCQiHYQ0XYi6ik/N4uIfkZE+8r3M8vPExF9rmzvM0R0vWM7Hyy/fx8RfTChTV8momPlTpr6OWM2EdEN5e+8v/xZMmjnXxHRi+XjuZ2I3uJ47VPlfe4hojc5nnc9D4hoaXnFrn1E9E0imhDDxg4i+gURPVtehP3u8vPWHE8fG207lpOIaDMRPV2283/5bZuIJpb/3l9+fUlc+w3Y+BUiet5xLK8tP5/a+DGGUsqaG4BmAM8BuAjc2/1pAMtTsOMggNlVzz0A4N7y43sB/F358VsA/BgAgVeS2lR+fhaAA+X7meXHMxPYdBOA6wHsrIVNADYD6Cp/5scAbjVo518B+HOX9y4v/48nAlha/t83+50HAL4F4L3lxw8B+GgMGxcAuL78eBqAvWVbrDmePjbadiwJwAXlx60ANpWPkeu2AXwMwEPlx+8F8M249huw8SsA3uXy/tTGj6mbbR77KgD7lVIHFLcE/gaAt6Vsk+ZtAL5afvxVAG93PP/PiikBmEFECwC8CcDPlFInlFIvA/gZgDfH3blS6j8AnKiFTeXX2pRS3YrP0n92bMuEnV68DcA3lFJnlFLPA9gPPgdcz4OyF/QGABtcvnMUG19SSm0rP34FwLMAFsGi4+ljoxdpHUullBoq/9lavimfbTuP8QYAN5dtiWS/IRu9SG38mMI2YV8EoNfxdx/8T+ZaoQD8lIi2Eq/VCgDzlFIvATzoAMwtP+9lcz2+iymbFpUf19LWO8uXtV/WIY4YdrYDOKmUGjFlZzkUcB3Yi7PyeFbZCFh2LImomYi2AzgGFrvnfLb9n/aUXz9VtqWm46jaRqWUPpafKR/LtUQ0sdrGkLbUY/xEwjZhd4tLpZG283ql1PUAbgXwcSK6yee9Xjan+V2i2lRrW78I4GIA1wJ4CcD/KT+fqp1EdAGA7wBYo5Qa9HtrRHuM2elio3XHUik1qpS6FkAB7GFf4bPtVOystpGIrgLwKQCXA1gJDq98Mk0bTWKbsPcB6HD8XQBwuN5GKKUOl++PAfgu+GQ9Wr7kQvn+WPntXjbX47uYsqmv/LgmtiqljpYH1nkAD4OPZxw7j4Mvi1uqno8MEbWCBfNrSqnHy09bdTzdbLTxWGqUUicBPAmOS3tt+z/tKb8+HRy6q8s4ctj45nK4SymlzgB4BPGPZU3HTyxqGcCPegMv1XcAPHmiJ0qurLMNUwFMczz+DTg2/lmMnVh7oPz49zF2omWzqky0PA+eZJlZfjwroW1LMHZS0phNALaU36snf95i0M4Fjsf3gGOpAHAlxk6YHQBPlnmeBwC+jbGTch+LYR+B46Drqp635nj62GjbsZwDYEb58WQAvwJwm9e2AXwcYydPvxXXfgM2LnAc63UA/taG8WPiltqOff4JbwFnADwH4L4U9n9R+eR5GsAubQM4DvhzAPvK9/ofSgD+oWzvDgCdjm19GDwJtB/AhxLa9Rj40vsc2EP4Y5M2AegEsLP8mS+gXLxmyM5Hy3Y8A+AHGCtO95X3uQeOTAKv86D8/9lctv/bACbGsPF3wJfKzwDYXr69xabj6WOjbcfyagBPle3ZCeB/+m0bwKTy3/vLr18U134DNv57+VjuBPAvqGTOpDZ+TN2k8lQQBCFn2BZjFwRBEBIiwi4IgpAzRNgFQRByhgi7IAhCzhBhFwRByBki7IIgCDlDhF0QBCFniLALgiDkjP8P3QXrPB4LKsUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### This cell should produce from scratch a plot showing a learning curve for a single agent.\n",
    "import connect\n",
    "env = connect.Connect(starting_player='x', verbose=False)\n",
    "n=750\n",
    "k=50\n",
    "a=1\n",
    "perf = get_performance(n, k, a)\n",
    "x_axis = np.zeros(shape=(k))\n",
    "\n",
    "for q in range(k):\n",
    "    x_axis[q] = (q+1)*(n)\n",
    "plot_graph(x_axis, perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT: How to submit.\n",
    "\n",
    "If any of the following instructions is not clear, please ask your tutors well ahead of the submission deadline.\n",
    "\n",
    "### Before you submit\n",
    "- We will not be able to mark your coursework if it takes more than 1 minutes to execute your entire notebook. That is, comment out (but do not delete) the code that you used to produce Plot 1 (i.e., learning curve averaged across many agents). Do **not** comment out the code that you use to produce a learning curve for a single agent (Exercise D).\n",
    "- Restart the kernel (_Kernel $\\rightarrow$ Restart & Run All_) and make sure that you can run all cells from top to bottom without any errors.\n",
    "- Make sure that your code is written in Python 3 (and not in Python 2!). You can check the Python version of the current session in the top-right corner below the Python logo.\n",
    "\n",
    "### Submission file\n",
    "- Please upload to Moodle a .zip file (**not** `.rar`, `.7z`, or any other archive format) that contains the completed Jupyter notebook (`ai4_connect_three.ipynb`) as well as the pre-computed figure(s). \n",
    "- **If** you change the `connect.py` file or write your own version of the environment, include the corresponding file in your submission, but give it any other name than `connect.py`. If you do not change its name, it will be overwritten  and we won't be able to execute your code! Make sure that you import the correct module when you rename your file, for example, use `import myConnect` if your file is called `myConnect.py`.\n",
    "- Do not include any identifying information. Not in the code cells, not in the file names, nowhere! Marking is anonymous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
